\section{Some curiosity about the two algorithms}

While discussing with my supervisors or coding the algorithms we have seen that the \textit{L*} or the \textit{NL*} algorithms can have some curious properties.

\subsection{Minimize an automaton with L*?}

If in a certain way, we merge the notion of Teacher and Learner L* together we can create an algorithm \textit{Minimizer} to calculate \textit{mDFA} of a general \textit{DFA}. The \textit{Minimizer} will be composed by the L* implementation and a second part able to compare two automata and provide as the Teacher a counter-example.

We have seen that if we create a \textit{Minimizer} the algorithm will be something like this:

\begin{algorithm}[h]
  \caption{Minimizer}
  \KwIn{An automaton $\A$}
  \KwOut{$\A$ minimized}

  \While{True}{
    Run L*\;
    \uIf{Membership Query}{
      Check if $\omega$ is accepted by L*;
    }
    \Else{
      Check the equivalence between the conjecture $\A'$ and the $\A$\;
      \uIf{Equivalence}{
        \Return $\A'$
      } \Else {
        Find a counter-example
      }
    }
  }
\end{algorithm}

% This algorithm is correct since \textit{L*} always return a \textit{mDFA} that recognizes a language $\U$ and the automaton passed in parameter can be see as the representation of a regular language $\U$, by \cref{th:kleene}.

% The algorithm terminates: \textit{L*} and verifying the equivalence of two automata terminate.

The time complexity is a bit more complicated to calculate. Let $N$ the size of the automaton passed in parameter and $n$ the size of its \textit{mDFA}. Then at most \textit{L*} will create $n$ conjectures, one per state of the \textit{mDFA}. So we will pass through the loop $n$ times. The complexity of the equivalence test between automata is at most:
\begin{itemize}
  \item $O(n)$ to make to complementation of an automaton of size $n$;
  \item $O(n \times m)$ to calculate the intersection of two automata of size $n$ and $m$;
  \item $O(n)$ to test if the automaton is empty or find a counter-example.
\end{itemize}
The equivalence ask for: $(\LA \cap \overline{\mathcal{L}(\A')}) \cup (\overline{\LA} \cap \mathcal{L}(\A'))$ taking $O(2 \times (N + n + N \times n)) = O(n \times N)$.

The Learner complexity is $O(n^3)$ because the size of the longest counter-example will be at most $n$\footnote{The counter-example will always be of least length}.

The overall complexity inside the loop is $O(n)*(O(n^3) + O(n \times N)) = O(n^4 + n \times N)$.

A good algorithm that we know to minimize an automaton takes $O(N \log(N))$ which is clearly better than the \textit{Minimizer} complexity, however the \textit{Minimizer} can be better if the automaton to minimize has a very huge number of states and its minimized version is very small.

We can also create an \textit{NL-Minimizer} but in this case we will work with \underline{non} deterministic automata and the operation of complementation on that kind of automata is an hard problem.

\subsection{Can we remove the Consistence check?}

While testing the performances of the Learners algorithms, I have tried to modified some of their properties to see their behavior. For example I have seen that \textit{Consistence} if we remove the consistence check on the \textit{NL*} algorithm the algorithm seems to terminates on every expression of the benchmark sample, even if I do not have a concrete proof. I think that adding the counter-example into $E$ will autonomously allow to add the necessary columns to find the \textit{Prime} residuals.

In fact the consistence check allows to add new columns, but if we do not add this constraints, we will send conjectures when the table is only closed and the next counter-example will allow to spot new residuals.

The main drawback of this approach is that the Learner will have to make more Equivalence queries and the Teacher will have to work harder.
